{"cells":[{"cell_type":"code","execution_count":1,"id":"775a3284-9d9f-4d0e-9596-c9f67e3a37df","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["import os\n","import json\n","import uuid\n","import pandas as pd\n","import numpy as np\n","\n","\n","from typing import Dict, List"]},{"cell_type":"code","execution_count":2,"id":"b361fd44-b46f-4e98-a68f-8819aba4f5ba","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["input_path = 'raw/'  # where the original model.bim and report.json are\n","output_path = 'curated/'  # where the central file, the new model.bim and the new report.json will be generated\n","\n","report_name = 'FINMA Report'  # name of the report\n","original_language = 'DE'  # original languages code\n","\n","# do not change\n","language_table = 'LanguageSelection'  # the table holding the different languages code\n","language_table_column_code = 'LanguageCode'  # the column with the languages code\n","titles_table = 'TitlesTranslations'  # the table holding the title measures\n","\n","field_parameters_prefix = 'ZZ FP'  # prefix to use for field parameters, allow the code to seperate translation FP from business FP\n","\n","\n","# dictionary to hold all the argument\n","params = {\n","    'input_path': input_path,\n","    'output_path': output_path,\n","    'report_name': report_name,\n","    'original_language': original_language,\n","    'field_parameters_prefix': field_parameters_prefix,\n","    'language_table': language_table,\n","    'language_table_column_code': language_table_column_code,\n","    'titles_table': titles_table\n","}\n"]},{"cell_type":"code","execution_count":3,"id":"f15c59a8-ed1e-42f9-b623-151429cb99d5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["class TranslationGlobalTools:\n","    \"\"\"\n","    This class is just here to collect all arguments from the classes below\n","    and 2 functions to open the report.json and model.bim\n","    \"\"\"\n","    def __init__(self, params: Dict[str, str]) -> None:\n","        self.report_name = params.get('report_name', None)\n","        self.original_language = params.get('original_language', None)\n","\n","        self.input_path = params.get('input_path', None) + self.report_name + '/'\n","        self.output_path = params.get('output_path', None) + self.report_name\n","        self.machine_input_path = 'dummy-artifacts/'\n","        \n","        self.field_parameters_prefix = params.get('field_parameters_prefix', None)\n","\n","        self.language_table = params.get('language_table', None)\n","        self.language_table_column_code = params.get('language_table_column_code', None)\n","        self.titles_table = params.get('titles_table', None)\n","        \n","        self.metadata_path = f'{self.output_path}/metadata_new.csv'  # path of metadata and data that would need translations\n","        self.title_path = f'{self.output_path}/title_new.csv'  # path of titles that would need translations\n","        self.existing_field_parameters_path = f'{self.output_path}/translation_old.csv'  # path for already existing metadata and data translations\n","        self.existing_titles_path = f'{self.output_path}/title_old.csv'  # path for already existing title translations\n","        self.central_file_path = f'{self.output_path}/central.xlsx'  # path where the central file will be saved\n","\n","        if not os.path.exists(self.output_path):\n","            os.mkdir(self.output_path)\n","\n","        print('TranslationGlobalTools initiated.')\n","\n","    \n","    def open_report_json(self) -> dict:  # to open the original report.json file\n","        with open(self.input_path + 'report.json') as f:\n","            report_json = json.load(f) #dict\n","        return report_json\n","\n","\n","    def open_model_bim(self) -> dict:  # to open the original model.bim file\n","        with open(self.input_path + 'model.bim') as f:\n","            model_bim = json.load(f) #dict\n","        return model_bim\n"]},{"cell_type":"code","execution_count":4,"id":"986ea56d-c70f-4e98-9019-d61aeddcb962","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["class TranslationExtractor(TranslationGlobalTools):\n","    \"\"\"\n","    Goal of this class is:\n","    -> to collect the measures / columns / titles in need of translations\n","    -> to collect existing translation of measures / columns / titles\n","    -> to merge the 2 collections into 1 central file\n","    \"\"\"\n","    def __init__(self, params: Dict[str, str]) -> None:\n","        super().__init__(params)\n","        print('TranslationExtractor initiated.')\n","\n","    def get_list_of_visuals(self) -> list:\n","        \"\"\" \n","        Goal of this function is:\n","        -> to return any visuals that can contains projections or titles\n","\n","        returns:  a list of visuals\n","        \"\"\"\n","        report_json = self.open_report_json()\n","        report_pages = report_json['sections']\n","        visuals_to_extract = []\n","\n","        for page in report_pages: #give dictionnaries\n","\n","            page_visuals = page['visualContainers'] # give list\n","\n","            for visual in page_visuals: # give dictionnaries\n","                visual_config = json.loads(visual['config']) #Load the config in dictionary\n","                singleVisual = visual_config.get('singleVisual', False)\n","\n","                if singleVisual: #Visual group are just group of visual not individual visual\n","                    if singleVisual.get('projections', False):\n","                        visuals_to_extract.append(singleVisual)\n","\n","        return visuals_to_extract\n","\n","\n","    def extract_metadata(self) -> list:\n","        \"\"\"\n","        This function's goal is to : \n","        -> extract all columns and measure that are used at visual level\n","        i.e. that would need a translation\n","\n","        returns: list of metadata/data used in the report with the original text\n","        \"\"\"\n","        visuals_to_extract = self.get_list_of_visuals()\n","        all_used_metadata = []\n","\n","        for visual in visuals_to_extract:\n","            visual_prototypequery = visual['prototypeQuery']\n","            sources_from = visual_prototypequery['From']\n","            source_dict = dict([(src['Name'], src['Entity']) for src in sources_from])  # the source \n","\n","            column_properties_dict = visual.get('columnProperties', dict())\n","\n","            for sub_el in visual_prototypequery['Select']:\n","\n","                is_measure = sub_el.get('Measure', False) != False\n","                is_column = sub_el.get('Column', False) != False\n","                is_aggregation = sub_el.get('Aggregation', False) != False\n","\n","                if is_measure:\n","                    source_key = sub_el['Measure']['Expression']['SourceRef']['Source']\n","                    source_property = sub_el['Measure']['Property']\n","                    visual_kind = 'Measure'\n","                    function_type = ''\n","                    possibility = 1\n","                \n","                if is_column:\n","                    source_key = sub_el['Column']['Expression']['SourceRef']['Source']\n","                    source_property = sub_el['Column']['Property']\n","                    visual_kind = 'Column'\n","                    function_type = ''\n","                    possibility = 1\n","\n","                if is_aggregation:\n","                    source_key = sub_el['Aggregation']['Expression']['Column']['Expression']['SourceRef']['Source']\n","                    source_property = sub_el['Aggregation']['Expression']['Column']['Property']\n","                    visual_kind = 'Aggregation'\n","                    function_type = sub_el['Aggregation']['Function']\n","                    possibility = 0\n","\n","                if is_measure or is_column or is_aggregation:\n","                    # if the displayName (if it exists) is not the same, use it\n","                    col_prop_key = source_dict[source_key] + '.' +  source_property \n","                    display_name = column_properties_dict.get(col_prop_key, dict()).get('displayName', False)\n","                    \n","                    used_name = source_property\n","                    if display_name != False:\n","                        used_name = display_name\n","\n","                    # to understand logic between NativeReferenceName, Property and displayName\n","                    #if used_name != sub_el['NativeReferenceName'] and possibility:\n","                    #    print(used_name, '----', display_name, '----', sub_el['NativeReferenceName'])\n","\n","\n","                    all_used_metadata.append((visual_kind, col_prop_key, used_name, function_type, possibility))\n","                else:\n","                    a = visual\n","                    raise 'Unrecognized Visual Type.'\n","\n","        return all_used_metadata\n","\n","    def extract_titles(self) -> list:\n","        \"\"\"\n","        This function's goal is to : \n","        -> extract all titles that are used at visual level\n","        i.e. that would need a translation\n","\n","        returns: list of title used in the report\n","        \"\"\"\n","        visuals_to_extract = self.get_list_of_visuals()\n","        info = [visual.get('vcObjects', None) for visual in visuals_to_extract]\n","        info = [visual.get('title', None) for visual in info if visual]\n","        titles = [el[0]['properties'] for el in info if el]\n","\n","        titles_name = []\n","\n","        for title in titles:\n","            temp = title.get('text', None)\n","            if temp is not None:\n","                title_info = title['text']['expr']\n","                if title_info.get('Literal', False):\n","                    title_text = title_info['Literal']['Value']\n","                    title_text = title_text.strip(title_text[0])\n","                    titles_name.append(title_text)\n","\n","        return list(set(titles_name))\n","\n","\n","    def save_translation_needed(self, replace: bool = False) -> None:\n","        metadata_list = self.extract_metadata()\n","        titles_list = self.extract_titles()\n","\n","        df_metadata = pd.DataFrame(metadata_list, columns=['Type', 'SemanticCompleteName', self.original_language, 'AggregationFunction', 'Possible'])\n","        df_metadata.drop_duplicates(['Type', 'SemanticCompleteName', self.original_language], inplace=True)\n","        df_metadata.sort_values(['Type', 'SemanticCompleteName'], inplace=True)\n","        df_metadata['SemanticEntityName'] = df_metadata['SemanticCompleteName'].map(lambda x: '.'.join(x.split('.')[1:]))\n","        df_metadata = df_metadata[['Type', 'SemanticCompleteName', 'SemanticEntityName', 'AggregationFunction', 'Possible', self.original_language]]\n","\n","        df_title = pd.DataFrame({self.original_language: list(set(titles_list))})\n","        df_title = df_title[df_title[self.original_language].str.strip() != \"\"]\n","\n","        error_str = ''\n","\n","        if not os.path.exists(self.metadata_path) or replace: \n","            df_metadata.to_csv(self.metadata_path)\n","        else:\n","            error_str += f'\\nFile for Metadata already exists in the given path: {self.metadata_path}'\n","\n","        if not os.path.exists(self.title_path) or replace:\n","            df_title.to_csv(self.title_path)\n","        else:\n","            error_str += f'\\nFile for Titles already exists in the given path: {self.title_path}'\n","        \n","        if error_str:\n","            raise Exception(error_str)\n","\n","\n","    def save_central_file(self, replace: bool=False) -> None:\n","        \n","        new_path = self.metadata_path\n","        df_metadata = pd.read_csv(new_path, index_col=0)\n","\n","        df_metadata = df_metadata[df_metadata['Possible'] == 1]\n","        df_metadata.drop(['AggregationFunction', 'Possible'], axis=1, inplace=True)\n","\n","        df_metadata = df_metadata[['Type', 'SemanticCompleteName', 'SemanticEntityName', self.original_language]]\n","                \n","        new_path = self.title_path\n","        df_titles = pd.read_csv(new_path, index_col=0)\n","\n","        if not os.path.exists(self.central_file_path) or replace: \n","            with pd.ExcelWriter(self.central_file_path) as writer:  \n","                df_metadata.to_excel(writer, sheet_name='Metadata')\n","                df_titles.to_excel(writer, sheet_name='Titles')\n","        else:\n","            raise Exception(f'\\nFile for Central File already exists in the given path: {self.central_file_path}')\n","\n","\n","    def generate_central_file(self, replace: bool=False) -> None:\n","        self.save_translation_needed(replace=replace)\n","        self.save_central_file(replace=replace)\n"]},{"cell_type":"code","execution_count":5,"id":"c3bea454-52f7-4841-bfe4-64c8b0c07da1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["class TranslationEntityCreator(TranslationGlobalTools):\n","    \"\"\"\n","    Goal of this class is:\n","    -> Open the central translation file\n","    -> Collect existing field parameters (FP) and relationship in the semantic model\n","    -> Delete existing FP and relationship\n","\n","    -> Create the relationships\n","    -> Create the FP\n","\n","    Deleting all and remaking all is simplifying the code and allow us to make sure we are always\n","    with the most up-to-date translation\n","    \"\"\"\n","\n","    def __init__(self, params: Dict[str, str]) -> None:\n","        super().__init__(params)\n","        print('TranslationEntityCreator initiated.')\n","\n","\n","    def create_relationship(self) -> List[Dict[str, str]]:\n","        \"\"\"\n","        This function's goal is:\n","        -> add missing relationships between FPs' and the language table\n","\n","        return only relationship with language table\n","        \"\"\"\n","        model = self.open_model_bim()\n","\n","        df_metadata = pd.read_excel(self.central_file_path, sheet_name='Metadata', index_col=0)\n","        df_metadata.fillna('', inplace=True)\n","\n","        new_relationships = []\n","\n","        for n, semantic_name in df_metadata.iterrows():\n","            \n","            row_info = dict(semantic_name)\n","\n","            type_ = row_info['Type']\n","            semantic_name = row_info['SemanticCompleteName'].replace('[', '').replace(']', '').replace('.', '_')\n","            table_name = f\"{self.field_parameters_prefix} {semantic_name} {type_}\"\n","                \n","            new_relationship = {\n","                                    \"name\": str(uuid.uuid4()),\n","                                    \"crossFilteringBehavior\": \"bothDirections\",\n","                                    \"fromCardinality\": \"one\",\n","                                    \"fromColumn\": self.language_table_column_code,\n","                                    \"fromTable\": \"LanguageSelection\",\n","                                    \"toColumn\": self.language_table_column_code,\n","                                    \"toTable\": table_name\n","                                }\n","\n","            new_relationships.append(new_relationship)\n","\n","        return new_relationships \n","\n","\n","    def create_field_parameters(self) -> List[Dict]:\n","        \"\"\"\n","        This function's goal is:\n","        -> seperate FPs related to translation work to the other tables\n","        -> create from scraft the FPs\n","\n","        return all the table old and new FPs (without the old FP)\n","        \"\"\"\n","\n","        with open(f'{self.machine_input_path}dummy_field_parameters.txt') as f:\n","            dummy_field_parameter = f.read() #dict\n","\n","        df_metadata = pd.read_excel(self.central_file_path, sheet_name='Metadata', index_col=0)\n","        df_metadata.fillna('', inplace=True)\n","\n","        self.existing_languages = [code for code in list(df_metadata.columns)[3:] if 'semantic' not in code]\n","\n","        new_field_parameters = []\n","\n","        # creating one field parameters per line of the dataframe\n","        for n, semantic_name in df_metadata.iterrows():\n","            \n","            row_info = dict(semantic_name)\n","\n","            type_ = row_info['Type']\n","            semantic_name = row_info['SemanticCompleteName'].replace('[', '').replace(']', '').replace('.', '_')\n","            table_full_name = f\"{self.field_parameters_prefix} {semantic_name}\"\n","\n","            temp_fp = dummy_field_parameter\n","            temp_fp = temp_fp.replace('{FULL_TABLE_NAME}', table_full_name)\n","            temp_fp = temp_fp.replace('{TYPE}', type_)\n","\n","            temp_fp = temp_fp.replace('{ID_ANO}', str(uuid.uuid4()))\n","            temp_fp = temp_fp.replace('{ID_TABLE}', str(uuid.uuid4()))\n","            temp_fp = temp_fp.replace('{ID_COL1}', str(uuid.uuid4()))\n","            temp_fp = temp_fp.replace('{ID_COL2}', str(uuid.uuid4()))\n","            temp_fp = temp_fp.replace('{ID_COL3}', str(uuid.uuid4()))\n","            temp_fp = temp_fp.replace('{ID_COL4}', str(uuid.uuid4()))\n","\n","            fp_dict = json.loads(temp_fp)\n","\n","            expression = ['{']\n","\n","            text_original_language = row_info[self.original_language]\n","\n","            for n_code, code in enumerate(self.existing_languages):\n","\n","                text = row_info[code]\n","                if not text: #If there is no translation available, we want to use the report default text\n","                    text = text_original_language\n","\n","                semantic_table = row_info['SemanticCompleteName'].split('.')[0]\n","                semantic_column = '.'.join(row_info['SemanticCompleteName'].split('.')[1:])\n","                \n","                # if a replacement exist \n","                if f'semantic{code}' in row_info.keys() and row_info.get(f'semantic{code}', '').strip() != '':\n","                    semantic_table = row_info[f'semantic{code}' ].split('.')[0]\n","                    semantic_column = '.'.join(row_info[f'semantic{code}' ].split('.')[1:])\n","                \n","                semantic_column = semantic_column.replace(']', ']]') #power bi specific naming convention\n","\n","                part = f\"    (\\\"{text}\\\", NAMEOF('{semantic_table}'[{semantic_column}]), {n_code}, \\\"{code}\\\"),\"\n","\n","                expression.append(part)\n","\n","            expression[-1] = expression[-1][:-1] #delete last coma \n","\n","            expression.append('}')\n","\n","            fp_dict['partitions'][0]['source']['expression'] = expression\n","\n","            new_field_parameters.append(fp_dict)\n","\n","\n","        return new_field_parameters\n","\n","\n","    def create_title_translations(self) -> List[Dict]:\n","\n","        language_table_name = self.language_table\n","        titles_table = self.titles_table\n","        column_name = self.language_table_column_code\n","        switch_formula = f'SWITCH(MIN({language_table_name}[{column_name}]),'\n","        original_code = self.original_language\n","    \n","        with open(f'{self.machine_input_path}title_table.txt') as f:\n","            translation_table = json.loads(f.read()) #dict\n","\n","        df_title = pd.read_excel(self.central_file_path, sheet_name='Titles', index_col=0)\n","        df_title.fillna('', inplace=True)\n","\n","        new_title_measure = []\n","\n","        for _, row_info in df_title.iterrows():\n","\n","            pivot = row_info[original_code]\n","            measure_name = pivot.replace('[', '').replace(']', '')\n","\n","            expression = [switch_formula]\n","\n","            for k, v in row_info.items():\n","                if k != original_code:  # according to the SWITCH() docs, we don't need the default language as an option, it will be the last argument\n","                    if v:  # use original text if no translation is available\n","                        expression.append(f'    \"{k}\", \"{v}\",',)\n","                    else:\n","                        expression.append(f'    \"{k}\", \"{pivot}\",',)\n","\n","            expression.append(f'    \"{pivot}\")')\n","\n","            title_dict ={\n","                'name': f'{measure_name}_title',\n","                'expression': expression,\n","                'lineageTag': str(uuid.uuid4()),\n","                }\n","\n","            new_title_measure.append(title_dict)\n","\n","        translation_table['measures'] = new_title_measure\n","\n","        return  [translation_table]\n","\n","\n","    def create_language_table(self) -> List[Dict]:\n","        \n","        with open(f'{self.machine_input_path}language_selection.txt') as f:\n","            language_table = json.loads(f.read()) #dict\n","\n","        rows = '{{\"' + '\"}, {\"'.join(self.existing_languages) + '\"}}'\n","        language_table['partitions'][0]['source']['expression'] = f'DATATABLE(\"LanguageCode\", STRING, {rows})'\n","        \n","        language_table\n","            \n","        return [language_table]\n","\n","\n","    def generate_new_model_bim(self, file_name: str = 'model.bim') -> None:\n","        \"\"\"\n","        This function's goal is to complete the missing relationship between FPs' and the language table\n","        \"\"\"\n","        new_relationships = self.create_relationship()\n","        field_parameters = self.create_field_parameters()\n","        titles_translations = self.create_title_translations()\n","        language_table = self.create_language_table()\n","\n","        model = self.open_model_bim()\n","\n","        model['model']['relationships'].extend(new_relationships)\n","        model['model']['tables'].extend(field_parameters + titles_translations + language_table)\n","\n","        with open(f'{self.output_path}/{file_name}', 'w', encoding='utf-8') as json_file:\n","            print(f'{self.output_path}/{file_name}')\n","            json.dump(model, json_file, indent=4, ensure_ascii=False)\n"]},{"cell_type":"code","execution_count":6,"id":"b89ace8d-f5ca-4deb-ab70-ee7d9b204f90","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["class TranslationEntityConnector(TranslationGlobalTools):\n","    \"\"\"\n","    Goal of this class is:\n","    -> Open the central translation file\n","    -> Collect all visual that would need title or metadata translation\n","    -> Connect the needed field parameters or title measure\n","\n","    Deleting all and remaking all is simplifying the code and allow us to make sure we are always\n","    with the most up-to-date translation\n","    \"\"\"\n","\n","    def __init__(self, params: Dict[str, str]) -> None:\n","        super().__init__(params)\n","        print('TranslationEntityConnector initiated.')\n","\n","\n","    def connect_title_to_measure(self) -> Dict:\n","        report_json = self.open_report_json()\n","        table_title = self.titles_table\n","        pages = report_json['sections']\n","\n","        report_pages_after_change = []\n","\n","        for n_page, page in enumerate(pages):\n","            \n","            visuals = page['visualContainers']\n","            visuals_after_change = []\n","            \n","            for n_visual, visual in enumerate(visuals):\n","                \n","                visual_config = json.loads(visual['config'])\n","                \n","                if 'singleVisual' in visual_config.keys(): \n","                    #singleVisual needed\n","                    singleVisual = visual_config['singleVisual']\n","                else:\n","                    visuals_after_change.append(visual)\n","                    continue\n","                \n","                if not singleVisual.get('projections', False):\n","                    visuals_after_change.append(visual)\n","                    continue\n","                \n","                if singleVisual.get('vcObjects', False) and singleVisual['vcObjects'].get('title', False): \n","                    #if there is not title, we can leave this visual\n","                    title = singleVisual['vcObjects'].get('title')[0]['properties']\n","                else:\n","                    visuals_after_change.append(visual)\n","                    continue\n","\n","                if title.get('text', False) and title['text']['expr'].get('Literal', False):  \n","                    #if this a hand written value\n","                    title_info = title['text']['expr']\n","                else:\n","                    visuals_after_change.append(visual)\n","                    continue\n","\n","                title_value = title_info['Literal']['Value']\n","                title_value = title_value.replace(title_value[0], '')\n","                if not title_value.strip():\n","                    visuals_after_change.append(visual)\n","                    continue\n","\n","                measure_name = title_value.replace('[', '').replace(']', '') + '_title' #the name of the measure\n","                measured_expression = {'Expression':{'SourceRef':{'Entity': table_title}},'Property': measure_name} #the right json format\n","                \n","                title['text']['expr'].pop('Literal') # take the old version out\n","                title['text']['expr']['Measure'] = measured_expression # add the measure connection\n","\n","                singleVisual['vcObjects'].get('title')[0]['properties'] = title\n","                visual_config['singleVisual'] = singleVisual\n","\n","                visual['config'] = json.dumps(visual_config, ensure_ascii=False)\n","\n","                visuals_after_change.append(visual)\n","\n","            page['visualContainers'] = visuals_after_change\n","            report_pages_after_change.append(page)\n","\n","        report_json['sections'] = report_pages_after_change\n","\n","        return report_json\n","\n","    def _add_field_parameter_to_visual(self, prototypeQuery: Dict, projections: Dict, column_properties_dict: Dict, fp_type_dict: Dict) -> Dict:\n","\n","        source_tables_dict = dict({(el['Name'], el['Entity']) for el in prototypeQuery['From']})\n","\n","        # every reference name are not the semantic name but a simplified version of it\n","        # build a dict to get the original version from the simplified one\n","        projections_table_dict = {}\n","\n","        for el in prototypeQuery['Select']:\n","            type_key = list(el.keys())[0]\n","            if type_key != 'Aggregation': # Aggregation are not supported on field parameters\n","                source_table = el[type_key]['Expression']['SourceRef']['Source']\n","                source_column = el[type_key]['Property']\n","                used_name = el['Name']\n","                projections_table_dict[used_name] = (source_tables_dict[source_table], source_column)\n","\n","        # create the field-parameters dictionnary\n","        queryFieldParametersByRole = {}\n","\n","        # For every dimension on the projections\n","        for key in projections.keys():\n","\n","            fp_mapping = []\n","\n","            # for every type of projections i.e. X, Y and tooltips\n","            for n, el in enumerate(projections[key]):\n","\n","                table, column = projections_table_dict.get(el['queryRef'], ('', ''))\n","                semantic_name = table + '.' + column\n","                displayName = column_properties_dict.get(semantic_name, dict()).get('displayName', False)\n","\n","                used_name = column\n","                if displayName != False:\n","                    used_name = displayName\n","\n","                # if the projection used as a field parameters equivalent\n","                if fp_type_dict.get(semantic_name, False):\n","                    type_key = fp_type_dict[semantic_name]\n","                    \n","                    table_name = semantic_name.replace('[', '').replace(']', '').replace('.', '_')\n","                    new_mapping = {\n","                        'index': n, \n","                        'length': 1, \n","                        'expr': \n","                            {'Column': \n","                                {'Expression': \n","                                    {\n","                                        'SourceRef': \n","                                            {'Entity': f'{self.field_parameters_prefix} {table_name} {type_key}'}\n","                                        }, \n","                                        'Property': f'{self.field_parameters_prefix} {table_name} {type_key}'}\n","                                        }\n","                                        \n","                            }\n","\n","                    fp_mapping.append(new_mapping)\n","\n","            queryFieldParametersByRole[key] = fp_mapping\n","\n","        return queryFieldParametersByRole # we return the formatted field parameter hashmap\n","\n","\n","    def connect_translations_to_fp(self, report_json: Dict) -> Dict:\n","        df_metadata = pd.read_excel(self.central_file_path, sheet_name='Metadata', index_col=0)\n","        df_metadata.fillna('', inplace=True)\n","\n","        pages = report_json['sections']\n","\n","        df_metadata['FP_KEY'] = df_metadata['SemanticCompleteName'] + '_' + df_metadata[self.original_language]\n","\n","        fp_type_dict = dict(zip(df_metadata['SemanticCompleteName'].tolist(), df_metadata['Type'].tolist())) \n","\n","        report_pages_after_change = []\n","\n","        for n_page, page in enumerate(pages):\n","\n","            visuals = page['visualContainers']\n","            visuals_after_change = []\n","            \n","            for n_visual, visual in enumerate(visuals):\n","                \n","                visual_config = json.loads(visual['config'])\n","                \n","                if 'singleVisual' in visual_config.keys():  #singleVisual needed\n","                    singleVisual = visual_config['singleVisual']\n","                else:\n","                    visuals_after_change.append(visual)\n","                    continue\n","                \n","                if singleVisual.get('projections', False): \n","                    \n","                    prototypeQuery = singleVisual['prototypeQuery']\n","                    projections = singleVisual['projections']\n","                else:\n","                    visuals_after_change.append(visual)\n","                    continue\n","\n","                # if the field parameters already exist we leave\n","                if singleVisual.get('queryFieldParametersByRole', False):\n","                    visuals_after_change.append(visual)\n","                    continue\n","\n","                column_properties_dict = singleVisual.get('columnProperties', dict())\n","\n","                queryFieldParametersByRole = self._add_field_parameter_to_visual(prototypeQuery, projections, column_properties_dict, fp_type_dict)\n","                visual_config['singleVisual']['queryFieldParametersByRole'] = queryFieldParametersByRole\n","                visual['config'] = json.dumps(visual_config, ensure_ascii=False)\n","\n","                visuals_after_change.append(visual)\n","\n","            page['visualContainers'] = visuals_after_change\n","            report_pages_after_change.append(page)\n","\n","        report_json['sections'] = report_pages_after_change\n","\n","        return report_json\n","\n","\n","    def create_if_need_filter_language(self, report_json: Dict) -> Dict:\n","        \n","        filters_text = report_json.get('filters', '')\n","\n","        new_filter = {\n","                    'name': 'Filtere324fc6402c259d3cfd5',\n","                    'expression': {'Column': {'Expression': {'SourceRef': {'Entity': 'LanguageSelection'}},\n","                    'Property': 'LanguageCode'}},\n","                    'filter': {'Version': 2,\n","                    'From': [{'Name': 'l', 'Entity': 'LanguageSelection', 'Type': 0}],\n","                    'Where': [{'Condition': {'In': {'Expressions': [{'Column': {'Expression': {'SourceRef': {'Source': 'l'}},\n","                            'Property': 'LanguageCode'}}],\n","                        'Values': [[{'Literal': {'Value': f\"'{self.original_language}'\"}}]]}}}]},\n","                    'type': 'Categorical',\n","                    'howCreated': 1,\n","                    'objects': {'general': [{'properties': {'requireSingleSelect': {'expr': {'Literal': {'Value': 'true'}}}}}]}\n","                        }\n","\n","        if filters_text:\n","\n","            filters = json.loads(filters_text)\n","\n","            for filter_ in filters:\n","                name = filter_.get('expression', dict()).get('Column', dict()).get('Expression', dict()).get('SourceRef', dict()).get('Entity', \"\")\n","                if self.language_table == name:\n","                    return report_json\n","\n","            filters.append(new_filter)\n","        else:\n","            filters = [new_filter]\n","\n","\n","        filters_text = json.dumps(filters, ensure_ascii=False)\n","        report_json['filters'] = filters_text\n","\n","        return report_json\n","\n","\n","    def create_new_report_json(self, file_name: str = 'report.json') -> None:\n","        \n","        report_json = self.connect_title_to_measure()\n","        report_json = self.connect_translations_to_fp(report_json)\n","        report_json = self.create_if_need_filter_language(report_json)\n","        \n","        with open(f'{self.output_path}/{file_name}', 'w', encoding='utf-8') as json_file:\n","            print(f'{self.output_path}/{file_name}')\n","            json.dump(report_json, json_file, indent=4, ensure_ascii=False)"]},{"cell_type":"code","execution_count":7,"id":"4063dd52-886f-45ab-83b9-94beb1a4dff0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["class Translator(TranslationExtractor, TranslationEntityCreator, TranslationEntityConnector):\n","    def __init__(self, params: Dict[str, str]) -> None:\n","        super().__init__(params)\n","        print('Everything initiated.')"]},{"cell_type":"code","execution_count":8,"id":"0a42d3af-ce41-4ab6-94a8-7db73afbfe58","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"name":"stdout","output_type":"stream","text":["TranslationGlobalTools initiated.\n","TranslationEntityConnector initiated.\n","TranslationEntityCreator initiated.\n","TranslationExtractor initiated.\n","Everything initiated.\n"]}],"source":["translator = Translator(params)"]},{"cell_type":"code","execution_count":9,"id":"a9fa9fe3-2ebb-426a-b3de-87b67f90743c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["NEED_CENTRAL = False\n","\n","if NEED_CENTRAL:\n","    translator.generate_central_file(replace=True)"]},{"cell_type":"code","execution_count":10,"id":"58a2341e-39f5-41a9-9753-5c87cb43eb38","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"name":"stdout","output_type":"stream","text":["curated/FINMA Report/model.bim\n"]}],"source":["NEED_MODEL = True\n","\n","if NEED_MODEL:\n","    translator.generate_new_model_bim()"]},{"cell_type":"code","execution_count":11,"id":"f4d4ccb3-fe05-48e5-993a-9d9655a4f036","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"name":"stdout","output_type":"stream","text":["curated/FINMA Report/report.json\n"]}],"source":["NEW_REPORT = True\n","\n","if NEW_REPORT:\n","    translator.create_new_report_json()"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"1ce9927a-0e21-4455-aba9-b4f69dc96094","default_lakehouse_name":"multi_language_files","default_lakehouse_workspace_id":"763949f3-1cf6-4322-9fbc-018dc2ddaae5"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":".pbi-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
